{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS224W Final Project.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "In this colab, we'll learn how to train the algorithm RotatE on a Knowledge Graph, and understand all steps from loading the dataset, to setting up the model and making predictions at the end.\n",
        "\n",
        "First of all, we start by downloading the `ogb` and `tensorboardX` modules.  The `ogb` module, which stands for [Open Graph Benchmark](https://ogb.stanford.edu/docs/home/), contains graph datasets that are widely used in Graph ML research to serve as benchmarks. The second module is `tensorboardX`, which provides logging utilities.\n",
        "\n",
        "The code of this colab can be found at [Graph_Neural_Network_for_biological_predictions](https://github.com/Zahra-Bakhtiari/Graph_Neural_Network_for_biological_predictions.git) and is based on the following repositories: [snap-stanford](https://github.com/snap-stanford/ogb.git) and [KnowledgeGraphEmbedding](https://github.com/DeepGraphLearning/KnowledgeGraphEmbedding). The original code is more complete and customizable; in here, we simplify it as to make it easier for the reader to train their first GNN on an heterogenous graph. You may achieve similar results by running the CLI commands described in `KnowledgeGraphEmbedding` repository. In this colab, we extract the relevant code to expose the training logic behind the scenes."
      ],
      "metadata": {
        "id": "OMD9RLVTCWjq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OBjojMMqktCE",
        "outputId": "c245d607-e022-46e1-ff96-1e10de0409b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ogb\n",
            "  Downloading ogb-1.3.2-py3-none-any.whl (78 kB)\n",
            "\u001b[K     |████████████████████████████████| 78 kB 2.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.19.5)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.15.0)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (4.62.3)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.24.3)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.1.5)\n",
            "Collecting outdated>=0.2.0\n",
            "  Downloading outdated-0.2.1-py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.10.0+cu111)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.7/dist-packages (from ogb) (1.0.1)\n",
            "Collecting littleutils\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from outdated>=0.2.0->ogb) (2.23.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.1.0)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (1.4.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.20.0->ogb) (3.0.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.6.0->ogb) (3.10.0.2)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->outdated>=0.2.0->ogb) (2021.10.8)\n",
            "Building wheels for collected packages: littleutils\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7048 sha256=f38454f4c1c71428f7c2d93e24ca86d7bcc4fb2d7a9d1a3827bf902bb373a04b\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/64/cd/32819b511a488e4993f2fab909a95330289c3f4e0f6ef4676d\n",
            "Successfully built littleutils\n",
            "Installing collected packages: littleutils, outdated, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.2 outdated-0.2.1\n",
            "Collecting tensorboardX\n",
            "  Downloading tensorboardX-2.4.1-py2.py3-none-any.whl (124 kB)\n",
            "\u001b[K     |████████████████████████████████| 124 kB 2.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (3.17.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tensorboardX) (1.19.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.8.0->tensorboardX) (1.15.0)\n",
            "Installing collected packages: tensorboardX\n",
            "Successfully installed tensorboardX-2.4.1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "!pip install ogb\n",
        "!pip install tensorboardX"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data\n"
      ],
      "metadata": {
        "id": "rCnOPm9wEN4o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "After making sure we have all required modules mentioned above, the first step is to load and understand the data.\n",
        "\n",
        "Let's use the `TrainDataset` and `TestDataset` classes that are defined in [stanford-snap dataloader](https://github.com/snap-stanford/ogb/blob/9d7a3f5e89640938f93b52388cfe259ae8fe3855/examples/linkproppred/biokg/dataloader.py). These are helper classes that will assist us in retrieve positive and negative samples from our original dataset. These classes inherit from `torch.utils.data.Dataset`, which you can read more about in the [official docs](https://pytorch.org/docs/stable/data.html)."
      ],
      "metadata": {
        "id": "9ecSS8nRDrUe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TrainDataset(Dataset):\n",
        "    def __init__(self, triples, nentity, nrelation, negative_sample_size, mode, count, true_head, true_tail, entity_dict):\n",
        "        self.len = len(triples['head'])\n",
        "        self.triples = triples\n",
        "        self.nentity = nentity\n",
        "        self.nrelation = nrelation\n",
        "        self.negative_sample_size = negative_sample_size\n",
        "        self.mode = mode\n",
        "        self.count = count\n",
        "        self.true_head = true_head\n",
        "        self.true_tail = true_tail\n",
        "        self.entity_dict = entity_dict\n",
        "        \n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        head, relation, tail = self.triples['head'][idx], self.triples['relation'][idx], self.triples['tail'][idx]\n",
        "        head_type, tail_type = self.triples['head_type'][idx], self.triples['tail_type'][idx]\n",
        "        positive_sample = [head + self.entity_dict[head_type][0], relation, tail + self.entity_dict[tail_type][0]]\n",
        "\n",
        "        subsampling_weight = self.count[(head, relation, head_type)] + self.count[(tail, -relation-1, tail_type)]\n",
        "        subsampling_weight = torch.sqrt(1 / torch.Tensor([subsampling_weight]))\n",
        "\n",
        "        if self.mode == 'head-batch':\n",
        "            negative_sample = torch.randint(self.entity_dict[head_type][0], self.entity_dict[head_type][1], (self.negative_sample_size,))\n",
        "        elif self.mode == 'tail-batch':\n",
        "            negative_sample = torch.randint(self.entity_dict[tail_type][0], self.entity_dict[tail_type][1], (self.negative_sample_size,))\n",
        "        else:\n",
        "            raise\n",
        "        positive_sample = torch.LongTensor(positive_sample)\n",
        "            \n",
        "        return positive_sample, negative_sample, subsampling_weight, self.mode\n",
        "    \n",
        "    @staticmethod\n",
        "    def collate_fn(data):\n",
        "        positive_sample = torch.stack([_[0] for _ in data], dim=0)\n",
        "        negative_sample = torch.stack([_[1] for _ in data], dim=0)\n",
        "        subsample_weight = torch.cat([_[2] for _ in data], dim=0)\n",
        "        mode = data[0][3]\n",
        "        return positive_sample, negative_sample, subsample_weight, mode\n",
        "    "
      ],
      "metadata": {
        "id": "7PMhAMHHAbej"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TestDataset(Dataset):\n",
        "    def __init__(self, triples, args, mode, random_sampling, entity_dict):\n",
        "        self.len = len(triples['head'])\n",
        "        self.triples = triples\n",
        "        self.nentity = args['nentity']\n",
        "        self.nrelation = args['nrelation']\n",
        "        self.mode = mode\n",
        "        self.random_sampling = random_sampling\n",
        "        if random_sampling:\n",
        "            self.neg_size = args['neg_size_eval_train']\n",
        "        self.entity_dict = entity_dict\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        head, relation, tail = self.triples['head'][idx], self.triples['relation'][idx], self.triples['tail'][idx]\n",
        "        head_type, tail_type = self.triples['head_type'][idx], self.triples['tail_type'][idx]\n",
        "        positive_sample = torch.LongTensor((head + self.entity_dict[head_type][0], relation, tail + self.entity_dict[tail_type][0]))\n",
        "\n",
        "        if self.mode == 'head-batch':\n",
        "            if not self.random_sampling:\n",
        "                negative_sample = torch.cat([torch.LongTensor([head + self.entity_dict[head_type][0]]), \n",
        "                        torch.from_numpy(self.triples['head_neg'][idx] + self.entity_dict[head_type][0])])\n",
        "            else:\n",
        "                negative_sample = torch.cat([torch.LongTensor([head + self.entity_dict[head_type][0]]), \n",
        "                        torch.randint(self.entity_dict[head_type][0], self.entity_dict[head_type][1], size=(self.neg_size,))])\n",
        "        elif self.mode == 'tail-batch':\n",
        "            if not self.random_sampling:\n",
        "                negative_sample = torch.cat([torch.LongTensor([tail + self.entity_dict[tail_type][0]]), \n",
        "                        torch.from_numpy(self.triples['tail_neg'][idx] + self.entity_dict[tail_type][0])])\n",
        "            else:\n",
        "                negative_sample = torch.cat([torch.LongTensor([tail + self.entity_dict[tail_type][0]]), \n",
        "                        torch.randint(self.entity_dict[tail_type][0], self.entity_dict[tail_type][1], size=(self.neg_size,))])\n",
        "\n",
        "        return positive_sample, negative_sample, self.mode\n",
        "    \n",
        "    @staticmethod\n",
        "    def collate_fn(data):\n",
        "        positive_sample = torch.stack([_[0] for _ in data], dim=0)\n",
        "        negative_sample = torch.stack([_[1] for _ in data], dim=0)\n",
        "        mode = data[0][2]\n",
        "\n",
        "        return positive_sample, negative_sample, mode\n",
        "    "
      ],
      "metadata": {
        "id": "kZOEi9xLEyUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We also extract the `BidirectionalOneShotIterator` class, which is useful to iterate both over our training/validation/test heads and tails simultaneously. This is useful for retrieving both positive and negative samples."
      ],
      "metadata": {
        "id": "tv9yhBVNQBGB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BidirectionalOneShotIterator(object):\n",
        "    def __init__(self, dataloader_head, dataloader_tail):\n",
        "        self.iterator_head = self.one_shot_iterator(dataloader_head)\n",
        "        self.iterator_tail = self.one_shot_iterator(dataloader_tail)\n",
        "        self.step = 0\n",
        "        \n",
        "    def __next__(self):\n",
        "        self.step += 1\n",
        "        if self.step % 2 == 0:\n",
        "            data = next(self.iterator_head)\n",
        "        else:\n",
        "            data = next(self.iterator_tail)\n",
        "        return data\n",
        "    \n",
        "    @staticmethod\n",
        "    def one_shot_iterator(dataloader):\n",
        "        '''\n",
        "        Transform a PyTorch Dataloader into python iterator\n",
        "        '''\n",
        "        while True:\n",
        "            for data in dataloader:\n",
        "                yield data"
      ],
      "metadata": {
        "id": "JTfguS0fQCRt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this colab, we'll be using the [`ogbl-biokg` dataset](https://ogb.stanford.edu/docs/linkprop/#ogbl-biokg), which is defined [Hu et al. (2005)](https://arxiv.org/pdf/2005.00687.pdf) and contains data from a large number of biomedical data repositories.\n",
        "\n",
        "To download and explore the dataset, we may use the `ogb` module we previously installed."
      ],
      "metadata": {
        "id": "3QUv9tx-FBzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from ogb.linkproppred import LinkPropPredDataset\n",
        "\n",
        "dataset = LinkPropPredDataset(name='ogbl-biokg', root='drive/MyDrive/data')"
      ],
      "metadata": {
        "id": "E_o0oHtAFA5H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a514e47-99a9-4338-a7f7-59efbdff8ed7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://snap.stanford.edu/ogb/data/linkproppred/biokg.zip\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloaded 0.90 GB: 100%|██████████| 920/920 [06:31<00:00,  2.35it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting drive/MyDrive/data/biokg.zip\n",
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 3563.55it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a dataset with only one graph:"
      ],
      "metadata": {
        "id": "llUSBEXFGXC3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph = dataset[0]"
      ],
      "metadata": {
        "id": "LnFmGuVhFTEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `ogbl-kg` graph contains 5 types of entities:  diseases (10,687 nodes), proteins (17,499), drugs (10,533 nodes), side effects (9,969 nodes), and protein functions (45,085 nodes). \n",
        "\n",
        "We can explore this by checking its `num_nodes_dict`"
      ],
      "metadata": {
        "id": "gfhqCRbwGboW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "graph['num_nodes_dict']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VQ-Pvk-1F0ng",
        "outputId": "ab972506-bff0-449c-bcd2-bb3d738d2359"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'disease': 10687,\n",
              " 'drug': 10533,\n",
              " 'function': 45085,\n",
              " 'protein': 17499,\n",
              " 'sideeffect': 9969}"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We may also explore what kinds of relationships this knowledge graph has. In total, the description of the dataset mentions it has 51 types of directed relations. We can check that by print its `edge_reltype` dict:"
      ],
      "metadata": {
        "id": "4QAHQqq1GvRJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(graph['edge_reltype'].keys())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-gRM72aGgHZ",
        "outputId": "b340ecf8-71d9-4bcd-ae30-46a8ce352d37"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('disease', 'disease-protein', 'protein'),\n",
              " ('drug', 'drug-disease', 'disease'),\n",
              " ('drug', 'drug-drug_acquired_metabolic_disease', 'drug'),\n",
              " ('drug', 'drug-drug_bacterial_infectious_disease', 'drug'),\n",
              " ('drug', 'drug-drug_benign_neoplasm', 'drug'),\n",
              " ('drug', 'drug-drug_cancer', 'drug'),\n",
              " ('drug', 'drug-drug_cardiovascular_system_disease', 'drug'),\n",
              " ('drug', 'drug-drug_chromosomal_disease', 'drug'),\n",
              " ('drug', 'drug-drug_cognitive_disorder', 'drug'),\n",
              " ('drug', 'drug-drug_cryptorchidism', 'drug'),\n",
              " ('drug', 'drug-drug_developmental_disorder_of_mental_health', 'drug'),\n",
              " ('drug', 'drug-drug_endocrine_system_disease', 'drug'),\n",
              " ('drug', 'drug-drug_fungal_infectious_disease', 'drug'),\n",
              " ('drug', 'drug-drug_gastrointestinal_system_disease', 'drug'),\n",
              " ('drug', 'drug-drug_hematopoietic_system_disease', 'drug'),\n",
              " ('drug', 'drug-drug_hematopoietic_system_diseases', 'drug'),\n",
              " ('drug', 'drug-drug_hypospadias', 'drug'),\n",
              " ('drug', 'drug-drug_immune_system_disease', 'drug'),\n",
              " ('drug', 'drug-drug_inherited_metabolic_disorder', 'drug'),\n",
              " ('drug', 'drug-drug_integumentary_system_disease', 'drug'),\n",
              " ('drug', 'drug-drug_irritable_bowel_syndrome', 'drug'),\n",
              " ('drug', 'drug-drug_monogenic_disease', 'drug'),\n",
              " ('drug', 'drug-drug_musculoskeletal_system_disease', 'drug'),\n",
              " ('drug', 'drug-drug_nervous_system_disease', 'drug'),\n",
              " ('drug', 'drug-drug_orofacial_cleft', 'drug'),\n",
              " ('drug', 'drug-drug_parasitic_infectious_disease', 'drug'),\n",
              " ('drug', 'drug-drug_personality_disorder', 'drug'),\n",
              " ('drug', 'drug-drug_polycystic_ovary_syndrome', 'drug'),\n",
              " ('drug', 'drug-drug_pre-malignant_neoplasm', 'drug'),\n",
              " ('drug', 'drug-drug_psoriatic_arthritis', 'drug'),\n",
              " ('drug', 'drug-drug_reproductive_system_disease', 'drug'),\n",
              " ('drug', 'drug-drug_respiratory_system_disease', 'drug'),\n",
              " ('drug', 'drug-drug_sexual_disorder', 'drug'),\n",
              " ('drug', 'drug-drug_sleep_disorder', 'drug'),\n",
              " ('drug', 'drug-drug_somatoform_disorder', 'drug'),\n",
              " ('drug', 'drug-drug_struct_sim', 'drug'),\n",
              " ('drug', 'drug-drug_substance-related_disorder', 'drug'),\n",
              " ('drug', 'drug-drug_thoracic_disease', 'drug'),\n",
              " ('drug', 'drug-drug_urinary_system_disease', 'drug'),\n",
              " ('drug', 'drug-drug_viral_infectious_disease', 'drug'),\n",
              " ('drug', 'drug-protein', 'protein'),\n",
              " ('drug', 'drug-sideeffect', 'sideeffect'),\n",
              " ('function', 'function-function', 'function'),\n",
              " ('protein', 'protein-function', 'function'),\n",
              " ('protein', 'protein-protein_activation', 'protein'),\n",
              " ('protein', 'protein-protein_binding', 'protein'),\n",
              " ('protein', 'protein-protein_catalysis', 'protein'),\n",
              " ('protein', 'protein-protein_expression', 'protein'),\n",
              " ('protein', 'protein-protein_inhibition', 'protein'),\n",
              " ('protein', 'protein-protein_ptmod', 'protein'),\n",
              " ('protein', 'protein-protein_reaction', 'protein')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "G8Lqoy8uHlIr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We'll explore the `RotatE` and  `pRotatE` ([Sun et al. (2019)](https://arxiv.org/abs/1902.10197)) models. For more information and details on these models, refer to the linked paper and to our [blog post](https://medium.com/@seshwan2/de5acf0553ac)."
      ],
      "metadata": {
        "id": "DC7Eil1qHmoX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use the KGEModel defined in [KnowledgeGraphEmbedding](https://github.com/DeepGraphLearning/KnowledgeGraphEmbedding/blob/2e440e0f9c687314d5ff67ead68ce985dc446e3a/codes/model.py).\n",
        "\n",
        "This class defines a knowledge-graph oriented [`torch.nn.Module`](https://pytorch.org/docs/stable/generated/torch.nn.Module.html). We have the losses functions of TransE, RotatE and pRotatE defined here as well. The class is below, please take your time to understand how the code is structured."
      ],
      "metadata": {
        "id": "isGDuE9nID4k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/python3\n",
        "\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from sklearn.metrics import average_precision_score\n",
        "from torch.utils.data import DataLoader\n",
        "from collections import defaultdict\n",
        "\n",
        "from ogb.linkproppred import Evaluator\n",
        "\n",
        "\n",
        "class KGEModel(nn.Module):\n",
        "    def __init__(self, model_name, nentity, nrelation, hidden_dim, gamma, evaluator,\n",
        "                 double_entity_embedding=False, double_relation_embedding=False):\n",
        "        super(KGEModel, self).__init__()\n",
        "        self.model_name = model_name\n",
        "        self.nentity = nentity\n",
        "        self.nrelation = nrelation\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.epsilon = 2.0\n",
        "\n",
        "        self.gamma = nn.Parameter(\n",
        "            torch.Tensor([gamma]),\n",
        "            requires_grad=False\n",
        "        )\n",
        "\n",
        "        self.embedding_range = nn.Parameter(\n",
        "            torch.Tensor([(self.gamma.item() + self.epsilon) / hidden_dim]),\n",
        "            requires_grad=False\n",
        "        )\n",
        "\n",
        "        self.entity_dim = hidden_dim * 2 if double_entity_embedding else hidden_dim\n",
        "        self.relation_dim = hidden_dim * 2 if double_relation_embedding else hidden_dim\n",
        "\n",
        "        self.entity_embedding = nn.Parameter(torch.zeros(nentity, self.entity_dim))\n",
        "        nn.init.uniform_(\n",
        "            tensor=self.entity_embedding,\n",
        "            a=-self.embedding_range.item(),\n",
        "            b=self.embedding_range.item()\n",
        "        )\n",
        "\n",
        "        self.relation_embedding = nn.Parameter(torch.zeros(nrelation, self.relation_dim))\n",
        "        nn.init.uniform_(\n",
        "            tensor=self.relation_embedding,\n",
        "            a=-self.embedding_range.item(),\n",
        "            b=self.embedding_range.item()\n",
        "        )\n",
        "\n",
        "        if model_name == 'pRotatE':\n",
        "            self.modulus = nn.Parameter(torch.Tensor([[0.5 * self.embedding_range.item()]]))\n",
        "\n",
        "        if model_name not in ['TransE', 'pRotatE', 'RotatE']:\n",
        "            raise ValueError('model %s not supported' % model_name)\n",
        "\n",
        "        if model_name == 'RotatE' and (not double_entity_embedding or double_relation_embedding):\n",
        "            raise ValueError('RotatE should use --double_entity_embedding')\n",
        "\n",
        "        self.evaluator = evaluator\n",
        "\n",
        "    def forward(self, sample, mode='single'):\n",
        "        '''\n",
        "        Forward function that calculate the score of a batch of triples.\n",
        "        In the 'single' mode, sample is a batch of triple.\n",
        "        In the 'head-batch' or 'tail-batch' mode, sample consists two part.\n",
        "        The first part is usually the positive sample.\n",
        "        And the second part is the entities in the negative samples.\n",
        "        Because negative samples and positive samples usually share two elements\n",
        "        in their triple ((head, relation) or (relation, tail)).\n",
        "        '''\n",
        "\n",
        "        if mode == 'single':\n",
        "            batch_size, negative_sample_size = sample.size(0), 1\n",
        "\n",
        "            head = torch.index_select(\n",
        "                self.entity_embedding,\n",
        "                dim=0,\n",
        "                index=sample[:, 0]\n",
        "            ).unsqueeze(1)\n",
        "\n",
        "            relation = torch.index_select(\n",
        "                self.relation_embedding,\n",
        "                dim=0,\n",
        "                index=sample[:, 1]\n",
        "            ).unsqueeze(1)\n",
        "\n",
        "            tail = torch.index_select(\n",
        "                self.entity_embedding,\n",
        "                dim=0,\n",
        "                index=sample[:, 2]\n",
        "            ).unsqueeze(1)\n",
        "\n",
        "        elif mode == 'head-batch':\n",
        "            tail_part, head_part = sample\n",
        "            batch_size, negative_sample_size = head_part.size(0), head_part.size(1)\n",
        "\n",
        "            head = torch.index_select(\n",
        "                self.entity_embedding,\n",
        "                dim=0,\n",
        "                index=head_part.view(-1)\n",
        "            ).view(batch_size, negative_sample_size, -1)\n",
        "\n",
        "            relation = torch.index_select(\n",
        "                self.relation_embedding,\n",
        "                dim=0,\n",
        "                index=tail_part[:, 1]\n",
        "            ).unsqueeze(1)\n",
        "\n",
        "            tail = torch.index_select(\n",
        "                self.entity_embedding,\n",
        "                dim=0,\n",
        "                index=tail_part[:, 2]\n",
        "            ).unsqueeze(1)\n",
        "\n",
        "        elif mode == 'tail-batch':\n",
        "            head_part, tail_part = sample\n",
        "            batch_size, negative_sample_size = tail_part.size(0), tail_part.size(1)\n",
        "\n",
        "            head = torch.index_select(\n",
        "                self.entity_embedding,\n",
        "                dim=0,\n",
        "                index=head_part[:, 0]\n",
        "            ).unsqueeze(1)\n",
        "\n",
        "            relation = torch.index_select(\n",
        "                self.relation_embedding,\n",
        "                dim=0,\n",
        "                index=head_part[:, 1]\n",
        "            ).unsqueeze(1)\n",
        "\n",
        "            tail = torch.index_select(\n",
        "                self.entity_embedding,\n",
        "                dim=0,\n",
        "                index=tail_part.view(-1)\n",
        "            ).view(batch_size, negative_sample_size, -1)\n",
        "\n",
        "        else:\n",
        "            raise ValueError('mode %s not supported' % mode)\n",
        "\n",
        "        model_func = {\n",
        "            'TransE': self.TransE,\n",
        "            'pRotatE': self.pRotatE,\n",
        "            'RotatE': self.RotatE,\n",
        "        }\n",
        "\n",
        "        if self.model_name in model_func:\n",
        "            score = model_func[self.model_name](head, relation, tail, mode)\n",
        "        else:\n",
        "            raise ValueError('model %s not supported' % self.model_name)\n",
        "\n",
        "        return score\n",
        "\n",
        "    def TransE(self, head, relation, tail, mode):\n",
        "        if mode == 'head-batch':\n",
        "            score = head + (relation - tail)\n",
        "        else:\n",
        "            score = (head + relation) - tail\n",
        "\n",
        "        score = self.gamma.item() - torch.norm(score, p=1, dim=2)\n",
        "        return score\n",
        "\n",
        "    def RotatE(self, head, relation, tail, mode):\n",
        "\n",
        "        pi = 3.14159265358979323846\n",
        "\n",
        "        re_head, im_head = torch.chunk(head, 2, dim=2)\n",
        "        re_tail, im_tail = torch.chunk(tail, 2, dim=2)\n",
        "\n",
        "        # Make phases of relations uniformly distributed in [-pi, pi]\n",
        "\n",
        "        phase_relation = relation / (self.embedding_range.item() / pi)\n",
        "\n",
        "        re_relation = torch.cos(phase_relation)\n",
        "        im_relation = torch.sin(phase_relation)\n",
        "\n",
        "        if mode == 'head-batch':\n",
        "            re_score = re_relation * re_tail + im_relation * im_tail\n",
        "            im_score = re_relation * im_tail - im_relation * re_tail\n",
        "            re_score = re_score - re_head\n",
        "            im_score = im_score - im_head\n",
        "        else:\n",
        "            re_score = re_head * re_relation - im_head * im_relation\n",
        "            im_score = re_head * im_relation + im_head * re_relation\n",
        "            re_score = re_score - re_tail\n",
        "            im_score = im_score - im_tail\n",
        "\n",
        "        score = torch.stack([re_score, im_score], dim=0)\n",
        "        score = score.norm(dim=0)\n",
        "\n",
        "        score = self.gamma.item() - score.sum(dim=2)\n",
        "        return score\n",
        "\n",
        "    def pRotatE(self, head, relation, tail, mode):\n",
        "        pi = 3.14159262358979323846\n",
        "\n",
        "        # Make phases of entities and relations uniformly distributed in [-pi, pi]\n",
        "\n",
        "        phase_head = head / (self.embedding_range.item() / pi)\n",
        "        phase_relation = relation / (self.embedding_range.item() / pi)\n",
        "        phase_tail = tail / (self.embedding_range.item() / pi)\n",
        "\n",
        "        if mode == 'head-batch':\n",
        "            score = phase_head + (phase_relation - phase_tail)\n",
        "        else:\n",
        "            score = (phase_head + phase_relation) - phase_tail\n",
        "\n",
        "        score = torch.sin(score)\n",
        "        score = torch.abs(score)\n",
        "\n",
        "        score = self.gamma.item() - score.sum(dim=2) * self.modulus\n",
        "        return score\n",
        "\n",
        "    @staticmethod\n",
        "    def train_step(model, optimizer, train_iterator, args):\n",
        "        '''\n",
        "        A single train step. Apply back-propation and return the loss\n",
        "        '''\n",
        "\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        positive_sample, negative_sample, subsampling_weight, mode = next(train_iterator)\n",
        "\n",
        "        positive_sample = positive_sample.cuda()\n",
        "        negative_sample = negative_sample.cuda()\n",
        "        subsampling_weight = subsampling_weight.cuda()\n",
        "\n",
        "        negative_score = model((positive_sample, negative_sample), mode=mode)\n",
        "        if args['negative_adversarial_sampling']:\n",
        "            # In self-adversarial sampling, we do not apply back-propagation on the sampling weight\n",
        "            negative_score = (F.softmax(negative_score * args['adversarial_temperature'], dim=1).detach()\n",
        "                              * F.logsigmoid(-negative_score)).sum(dim=1)\n",
        "        else:\n",
        "            negative_score = F.logsigmoid(-negative_score).mean(dim=1)\n",
        "\n",
        "        positive_score = model(positive_sample)\n",
        "        positive_score = F.logsigmoid(positive_score).squeeze(dim=1)\n",
        "        \n",
        "        positive_sample_loss = - (subsampling_weight * positive_score).sum() / subsampling_weight.sum()\n",
        "        negative_sample_loss = - (subsampling_weight * negative_score).sum() / subsampling_weight.sum()\n",
        "\n",
        "        loss = (positive_sample_loss + negative_sample_loss) / 2\n",
        "        regularization_log = {}\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        log = {\n",
        "            **regularization_log,\n",
        "            'positive_sample_loss': positive_sample_loss.item(),\n",
        "            'negative_sample_loss': negative_sample_loss.item(),\n",
        "            'loss': loss.item()\n",
        "        }\n",
        "\n",
        "        return log\n",
        "\n",
        "    @staticmethod\n",
        "    def test_step(model, test_triples, args, entity_dict, random_sampling=False):\n",
        "        '''\n",
        "        Evaluate the model on test or valid datasets\n",
        "        '''\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        # Prepare dataloader for evaluation\n",
        "        test_dataloader_head = DataLoader(\n",
        "            TestDataset(\n",
        "                test_triples,\n",
        "                args,\n",
        "                'head-batch',\n",
        "                random_sampling,\n",
        "                entity_dict\n",
        "            ),\n",
        "            batch_size=args['test_batch_size'],\n",
        "            num_workers=max(1, args['cpu_num'] // 2),\n",
        "            collate_fn=TestDataset.collate_fn\n",
        "        )\n",
        "\n",
        "        test_dataloader_tail = DataLoader(\n",
        "            TestDataset(\n",
        "                test_triples,\n",
        "                args,\n",
        "                'tail-batch',\n",
        "                random_sampling,\n",
        "                entity_dict\n",
        "            ),\n",
        "            batch_size=args['test_batch_size'],\n",
        "            num_workers=max(1, args['cpu_num'] // 2),\n",
        "            collate_fn=TestDataset.collate_fn\n",
        "        )\n",
        "\n",
        "        test_dataset_list = [test_dataloader_head, test_dataloader_tail]\n",
        "\n",
        "        test_logs = defaultdict(list)\n",
        "\n",
        "        step = 0\n",
        "        total_steps = sum([len(dataset) for dataset in test_dataset_list])\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for test_dataset in test_dataset_list:\n",
        "                for positive_sample, negative_sample, mode in test_dataset:\n",
        "                    positive_sample = positive_sample.cuda()\n",
        "                    negative_sample = negative_sample.cuda()\n",
        "\n",
        "                    batch_size = positive_sample.size(0)\n",
        "                    score = model((positive_sample, negative_sample), mode)\n",
        "\n",
        "                    batch_results = model.evaluator.eval({'y_pred_pos': score[:, 0],\n",
        "                                                          'y_pred_neg': score[:, 1:]})\n",
        "                    for metric in batch_results:\n",
        "                        test_logs[metric].append(batch_results[metric])\n",
        "\n",
        "                    if step % args['test_log_steps'] == 0:\n",
        "                        print('Evaluating the model... (%d/%d)' % (step, total_steps))\n",
        "\n",
        "                    step += 1\n",
        "\n",
        "            metrics = {}\n",
        "            for metric in test_logs:\n",
        "                metrics[metric] = torch.cat(test_logs[metric]).mean().item()\n",
        "\n",
        "        return metrics"
      ],
      "metadata": {
        "id": "MezI00RuIBqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have our data and our models defined. We can proceed to train our models and get some results!\n",
        "\n",
        "The code from the training section is also based on the [KnowledgeGraphEmbedding](https://github.com/DeepGraphLearning/KnowledgeGraphEmbedding/) repository. As aforementioned, the original code allows for multiple customizations and hyperparameter, which we summarize in the `args` dictionary below. Feel free to play and change these different values to see how they impact convergence and accurancy of the models!\n",
        "\n",
        "For the full list of arguments, please refer to the [original file](https://github.com/DeepGraphLearning/KnowledgeGraphEmbedding/blob/master/codes/run.py) where these are defined."
      ],
      "metadata": {
        "id": "PLm9VrxxJCyJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `args` dictionary paramets are defined below, which a short explanationL\n",
        "\n",
        "*   `dataset`: The dataset name.\n",
        "*   `model`: The model being trained.\n",
        "*   `hidden_dim`: The embeddings' dimension.\n",
        "*   `gamma`: The value for `gamma` (refer to the model loss functions).\n",
        "*   `batch_size`: The size of each batch.\n",
        "*   `negative_sample_size`: The size of each negative sample.\n",
        "*   `adversarial_temperature`: The temperature value for `alpha` used in Sun et al (2019) for negative sampling.\n",
        "*   `learning_rate`: The learning rate.\n",
        "*   `max_steps`: The maximum number of iterations for the training.\n",
        "*   `cpu_num`: Number of CPUs.\n",
        "*   `test_batch_size`: The size of each validation and test sets batch.\n",
        "*   `neg_size_eval_train`: The number of negative samples when evaluating training.\n",
        "*   `save_checkpoint_steps`: The number of steps until saving a checkpoint for the model.\n",
        "*   `valid_steps`: The number of steps until evaluating the model.\n",
        "*   `log_steps`: The number of steps until logging training data.\n",
        "*   `double_entity_embedding`: Whether we have double entity embeddings.\n",
        "*   `double_relation_embedding`: Whether we have double relation embeddings.\n",
        "*   `negative_adversarial_sampling`: Whether we want negative adversarial sampling."
      ],
      "metadata": {
        "id": "rieIVx3TKKH1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training\n"
      ],
      "metadata": {
        "id": "LYNCf7mGDiwU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're now ready to train our models. This code is based on [snap-stanford ogb repository](https://github.com/snap-stanford/ogb/blob/9d7a3f5e89640938f93b52388cfe259ae8fe3855/examples/linkproppred/biokg/run.py)"
      ],
      "metadata": {
        "id": "38Htinh_NbzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from ogb.linkproppred import Evaluator\n",
        "from collections import defaultdict\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "from tensorboardX import SummaryWriter\n",
        "import pdb\n",
        "\n",
        "def log_metrics(mode, step, metrics, writer):\n",
        "    '''\n",
        "    Print the evaluation logs\n",
        "    '''\n",
        "    for metric in metrics:\n",
        "        print('%s %s at step %d: %f' % (mode, metric, step, metrics[metric]))\n",
        "        writer.add_scalar(\"_\".join([mode, metric]), metrics[metric], step)\n",
        "\n",
        "def run(model, double_entity_embedding, hidden_dim=1000, gamma=20):\n",
        "  dataset_name = 'ogbl-biokg'\n",
        "  dataset = LinkPropPredDataset(name=dataset_name, root='drive/MyDrive/data')\n",
        "  \n",
        "  args = {'dataset': dataset_name,  \n",
        "          'model': model,          \n",
        "          'hidden_dim': hidden_dim,\n",
        "          'gamma': gamma,\n",
        "          'double_entity_embedding': double_entity_embedding,\n",
        "          'batch_size': 512,\n",
        "          'negative_sample_size': 128,\n",
        "          'adversarial_temperature': 1.0,\n",
        "          'learning_rate': 0.0001,\n",
        "          'max_steps': 300000,\n",
        "          'cpu_num': 2,\n",
        "          'test_batch_size': 32,\n",
        "          'neg_size_eval_train': 500,\n",
        "          'save_checkpoint_steps': 10000,\n",
        "          'valid_steps': 10000,\n",
        "          'log_steps': 100,\n",
        "          'double_relation_embedding': False,    \n",
        "          'negative_adversarial_sampling': True, \n",
        "          'do_train': True,\n",
        "          'do_test': True,\n",
        "          'do_valid': True,\n",
        "          'evaluate_train': True}\n",
        "  save_path = 'log/%s/%s/%s-%s/%s' % (args['dataset'],\n",
        "                                            args['model'], \n",
        "                                            args['hidden_dim'],\n",
        "                                            args['gamma'],\n",
        "                                            time.time())\n",
        "  writer = SummaryWriter(save_path)\n",
        "\n",
        "  split_edge = dataset.get_edge_split()\n",
        "  train_triples, valid_triples, test_triples = split_edge[\"train\"], split_edge[\"valid\"], split_edge[\"test\"]\n",
        "  nrelation = int(max(train_triples['relation'])) + 1\n",
        "  entity_dict = dict()\n",
        "  cur_idx = 0\n",
        "  for key in dataset[0]['num_nodes_dict']:\n",
        "      entity_dict[key] = (cur_idx, cur_idx + dataset[0]['num_nodes_dict'][key])\n",
        "      cur_idx += dataset[0]['num_nodes_dict'][key]\n",
        "  nentity = sum(dataset[0]['num_nodes_dict'].values())\n",
        "\n",
        "  args['nentity'] = nentity\n",
        "  args['nrelation'] = nrelation\n",
        "\n",
        "  evaluator = Evaluator(name=dataset_name)\n",
        "  train_count, train_true_head, train_true_tail = defaultdict(lambda: 4), defaultdict(list), defaultdict(list)\n",
        "\n",
        "  for i in tqdm(range(len(train_triples['head']))):\n",
        "      head, relation, tail = train_triples['head'][i], train_triples['relation'][i], train_triples['tail'][i]\n",
        "      head_type, tail_type = train_triples['head_type'][i], train_triples['tail_type'][i]\n",
        "      train_count[(head, relation, head_type)] += 1\n",
        "      train_count[(tail, -relation - 1, tail_type)] += 1\n",
        "      train_true_head[(relation, tail)].append(head)\n",
        "      train_true_tail[(head, relation)].append(tail)\n",
        "\n",
        "  kge_model = KGEModel(\n",
        "      model_name=args['model'],\n",
        "      nentity=nentity,\n",
        "      nrelation=nrelation,\n",
        "      hidden_dim=args['hidden_dim'],\n",
        "      gamma=args['gamma'],\n",
        "      double_entity_embedding=args['double_entity_embedding'],\n",
        "      double_relation_embedding=args['double_relation_embedding'],\n",
        "      evaluator=evaluator\n",
        "  )\n",
        "  \n",
        "  kge_model = kge_model.cuda()\n",
        "\n",
        "  if args['do_train']:\n",
        "      train_dataloader_head = DataLoader(\n",
        "          TrainDataset(train_triples, nentity, nrelation,\n",
        "                        args['negative_sample_size'], 'head-batch',\n",
        "                        train_count, train_true_head, train_true_tail,\n",
        "                        entity_dict),\n",
        "          batch_size=args['batch_size'],\n",
        "          shuffle=True,\n",
        "          num_workers=max(1, args['cpu_num'] // 2),\n",
        "          collate_fn=TrainDataset.collate_fn\n",
        "      )\n",
        "\n",
        "      train_dataloader_tail = DataLoader(\n",
        "          TrainDataset(train_triples, nentity, nrelation,\n",
        "                        args['negative_sample_size'], 'tail-batch',\n",
        "                        train_count, train_true_head, train_true_tail,\n",
        "                        entity_dict),\n",
        "          batch_size=args['batch_size'],\n",
        "          shuffle=True,\n",
        "          num_workers=max(1, args['cpu_num'] // 2),\n",
        "          collate_fn=TrainDataset.collate_fn\n",
        "      )\n",
        "\n",
        "      train_iterator = BidirectionalOneShotIterator(train_dataloader_head, train_dataloader_tail)\n",
        "\n",
        "      current_learning_rate = args['learning_rate']\n",
        "      optimizer = torch.optim.Adam(\n",
        "          filter(lambda p: p.requires_grad, kge_model.parameters()),\n",
        "          lr=current_learning_rate\n",
        "      )\n",
        "      warm_up_steps = args['max_steps'] // 2\n",
        "\n",
        "      print('Ramdomly Initializing %s Model...' % args['model'])\n",
        "      init_step = 0\n",
        "\n",
        "  step = init_step\n",
        "\n",
        "  print('Start Training...')\n",
        "  print('init_step = %d' % init_step)\n",
        "  print('batch_size = %d' % args['batch_size'])\n",
        "  print('negative_adversarial_sampling = %d' % args['negative_adversarial_sampling'])\n",
        "  print('hidden_dim = %d' % args['hidden_dim'])\n",
        "  print('gamma = %f' % args['gamma'])\n",
        "  print('negative_adversarial_sampling = %s' % str(args['negative_adversarial_sampling']))\n",
        "  print('adversarial_temperature = %f' % args['adversarial_temperature'])\n",
        "\n",
        "  # Set valid dataloader as it would be evaluated during training\n",
        "\n",
        "  if args['do_train']:\n",
        "      print('learning_rate = %d' % current_learning_rate)\n",
        "\n",
        "      training_logs = []\n",
        "\n",
        "      # Training Loop\n",
        "      for step in range(init_step, args['max_steps']):\n",
        "\n",
        "          log = kge_model.train_step(kge_model, optimizer, train_iterator, args)\n",
        "          training_logs.append(log)\n",
        "\n",
        "          if step >= warm_up_steps:\n",
        "              current_learning_rate = current_learning_rate / 10\n",
        "              print('Change learning_rate to %f at step %d' % (current_learning_rate, step))\n",
        "              optimizer = torch.optim.Adam(\n",
        "                  filter(lambda p: p.requires_grad, kge_model.parameters()),\n",
        "                  lr=current_learning_rate\n",
        "              )\n",
        "              warm_up_steps = warm_up_steps * 3\n",
        "\n",
        "          if step % args['save_checkpoint_steps'] == 0 and step > 0:  # ~ 41 seconds/saving\n",
        "              save_variable_list = {\n",
        "                  'step': step,\n",
        "                  'current_learning_rate': current_learning_rate,\n",
        "                  'warm_up_steps': warm_up_steps,\n",
        "                  'entity_dict': entity_dict\n",
        "              }\n",
        "              save_model(kge_model, optimizer, save_variable_list, args)\n",
        "\n",
        "          if step % args['log_steps'] == 0:\n",
        "              metrics = {}\n",
        "              for metric in training_logs[0].keys():\n",
        "                  metrics[metric] = sum([log[metric] for log in training_logs]) / len(training_logs)\n",
        "              log_metrics('Train', step, metrics, writer)\n",
        "              training_logs = []\n",
        "\n",
        "          if args['do_valid'] and step % args['valid_steps'] == 0 and step > 0:\n",
        "              print('Evaluating on Valid Dataset...')\n",
        "              metrics = kge_model.test_step(kge_model, valid_triples, args, entity_dict)\n",
        "              log_metrics('Valid', step, metrics, writer)\n",
        "\n",
        "      save_variable_list = {\n",
        "          'step': step,\n",
        "          'current_learning_rate': current_learning_rate,\n",
        "          'warm_up_steps': warm_up_steps\n",
        "      }\n",
        "      save_model(kge_model, optimizer, save_variable_list, args)\n",
        "\n",
        "  if args['do_valid']:\n",
        "      print('Evaluating on Valid Dataset...')\n",
        "      metrics = kge_model.test_step(kge_model, valid_triples, args, entity_dict)\n",
        "      log_metrics('Valid', step, metrics, writer)\n",
        "\n",
        "  if args['do_test']:\n",
        "      print('Evaluating on Test Dataset...')\n",
        "      metrics = kge_model.test_step(kge_model, test_triples, args, entity_dict)\n",
        "      log_metrics('Test', step, metrics, writer)\n",
        "\n",
        "  if args['evaluate_train']:\n",
        "      print('Evaluating on Training Dataset...')\n",
        "      small_train_triples = {}\n",
        "      indices = np.random.choice(len(train_triples['head']), args['ntriples_eval_train'], replace=False)\n",
        "      for i in train_triples:\n",
        "          if 'type' in i:\n",
        "              small_train_triples[i] = [train_triples[i][x] for x in indices]\n",
        "          else:\n",
        "              small_train_triples[i] = train_triples[i][indices]\n",
        "      metrics = kge_model.test_step(kge_model, small_train_triples, args, entity_dict, random_sampling=True)\n",
        "      log_metrics('Train', step, metrics, writer)"
      ],
      "metadata": {
        "id": "tzYMPUa8rZgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can finally start our training:"
      ],
      "metadata": {
        "id": "5FIquW6EP5UQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "run(model = 'RotatE', double_entity_embedding = True)"
      ],
      "metadata": {
        "id": "pi5o32ow1FdO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 583
        },
        "outputId": "ef0986ad-d65c-47e9-ab38-37fcd5bd19d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4762678/4762678 [00:15<00:00, 302814.74it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ramdomly Initializing RotatE Model...\n",
            "Start Training...\n",
            "init_step = 0\n",
            "batch_size = 512\n",
            "negative_adversarial_sampling = 1\n",
            "hidden_dim = 1000\n",
            "gamma = 20.000000\n",
            "negative_adversarial_sampling = True\n",
            "adversarial_temperature = 1.000000\n",
            "learning_rate = 0\n",
            "Train positive_sample_loss at step 0: 2.989598\n",
            "Train negative_sample_loss at step 0: 0.058889\n",
            "Train loss at step 0: 1.524244\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-258c7d4d7e52>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'RotatE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdouble_entity_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-b382158aee00>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(model, double_entity_embedding, hidden_dim, gamma)\u001b[0m\n\u001b[1;32m    147\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'max_steps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m           \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkge_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkge_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_iterator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m           \u001b[0mtraining_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-f6c400b9d2c8>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(model, optimizer, train_iterator, args)\u001b[0m\n\u001b[1;32m    255\u001b[0m         log = {\n\u001b[1;32m    256\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mregularization_log\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m             \u001b[0;34m'positive_sample_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpositive_sample_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m             \u001b[0;34m'negative_sample_loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnegative_sample_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m             \u001b[0;34m'loss'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "run(model = 'pRotatE', double_entity_embedding = False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Or1o8G1WLzWT",
        "outputId": "c86e3cad-3aea-451b-e992-8f9635115930"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4762678/4762678 [00:18<00:00, 259242.00it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ramdomly Initializing pRotatE Model...\n",
            "Start Training...\n",
            "init_step = 0\n",
            "batch_size = 512\n",
            "negative_adversarial_sampling = 1\n",
            "hidden_dim = 1000\n",
            "gamma = 20.000000\n",
            "negative_adversarial_sampling = True\n",
            "adversarial_temperature = 1.000000\n",
            "learning_rate = 0\n",
            "Train positive_sample_loss at step 0: 0.000002\n",
            "Train negative_sample_loss at step 0: 13.008783\n",
            "Train loss at step 0: 6.504393\n",
            "Train positive_sample_loss at step 100: 0.000218\n",
            "Train negative_sample_loss at step 100: 9.806888\n",
            "Train loss at step 100: 4.903553\n",
            "Train positive_sample_loss at step 200: 0.090560\n",
            "Train negative_sample_loss at step 200: 3.627894\n",
            "Train loss at step 200: 1.859227\n",
            "Train positive_sample_loss at step 300: 0.615183\n",
            "Train negative_sample_loss at step 300: 0.763102\n",
            "Train loss at step 300: 0.689143\n",
            "Train positive_sample_loss at step 400: 0.647756\n",
            "Train negative_sample_loss at step 400: 0.665541\n",
            "Train loss at step 400: 0.656649\n",
            "Train positive_sample_loss at step 500: 0.641103\n",
            "Train negative_sample_loss at step 500: 0.655639\n",
            "Train loss at step 500: 0.648371\n",
            "Train positive_sample_loss at step 600: 0.631630\n",
            "Train negative_sample_loss at step 600: 0.648920\n",
            "Train loss at step 600: 0.640275\n",
            "Train positive_sample_loss at step 700: 0.619494\n",
            "Train negative_sample_loss at step 700: 0.636278\n",
            "Train loss at step 700: 0.627886\n",
            "Train positive_sample_loss at step 800: 0.611812\n",
            "Train negative_sample_loss at step 800: 0.626158\n",
            "Train loss at step 800: 0.618985\n",
            "Train positive_sample_loss at step 900: 0.600935\n",
            "Train negative_sample_loss at step 900: 0.613346\n",
            "Train loss at step 900: 0.607141\n",
            "Train positive_sample_loss at step 1000: 0.591003\n",
            "Train negative_sample_loss at step 1000: 0.602327\n",
            "Train loss at step 1000: 0.596665\n",
            "Train positive_sample_loss at step 1100: 0.584138\n",
            "Train negative_sample_loss at step 1100: 0.594256\n",
            "Train loss at step 1100: 0.589197\n",
            "Train positive_sample_loss at step 1200: 0.575619\n",
            "Train negative_sample_loss at step 1200: 0.583759\n",
            "Train loss at step 1200: 0.579689\n",
            "Train positive_sample_loss at step 1300: 0.569129\n",
            "Train negative_sample_loss at step 1300: 0.575846\n",
            "Train loss at step 1300: 0.572488\n",
            "Train positive_sample_loss at step 1400: 0.561370\n",
            "Train negative_sample_loss at step 1400: 0.567949\n",
            "Train loss at step 1400: 0.564660\n",
            "Train positive_sample_loss at step 1500: 0.550372\n",
            "Train negative_sample_loss at step 1500: 0.558564\n",
            "Train loss at step 1500: 0.554468\n",
            "Train positive_sample_loss at step 1600: 0.544501\n",
            "Train negative_sample_loss at step 1600: 0.550459\n",
            "Train loss at step 1600: 0.547480\n",
            "Train positive_sample_loss at step 1700: 0.535713\n",
            "Train negative_sample_loss at step 1700: 0.544382\n",
            "Train loss at step 1700: 0.540047\n",
            "Train positive_sample_loss at step 1800: 0.532285\n",
            "Train negative_sample_loss at step 1800: 0.537136\n",
            "Train loss at step 1800: 0.534710\n",
            "Train positive_sample_loss at step 1900: 0.524436\n",
            "Train negative_sample_loss at step 1900: 0.533158\n",
            "Train loss at step 1900: 0.528797\n",
            "Train positive_sample_loss at step 2000: 0.519016\n",
            "Train negative_sample_loss at step 2000: 0.528531\n",
            "Train loss at step 2000: 0.523773\n",
            "Train positive_sample_loss at step 2100: 0.513152\n",
            "Train negative_sample_loss at step 2100: 0.522037\n",
            "Train loss at step 2100: 0.517595\n",
            "Train positive_sample_loss at step 2200: 0.509309\n",
            "Train negative_sample_loss at step 2200: 0.519470\n",
            "Train loss at step 2200: 0.514389\n",
            "Train positive_sample_loss at step 2300: 0.504385\n",
            "Train negative_sample_loss at step 2300: 0.514199\n",
            "Train loss at step 2300: 0.509292\n",
            "Train positive_sample_loss at step 2400: 0.496937\n",
            "Train negative_sample_loss at step 2400: 0.509485\n",
            "Train loss at step 2400: 0.503211\n",
            "Train positive_sample_loss at step 2500: 0.494528\n",
            "Train negative_sample_loss at step 2500: 0.506457\n",
            "Train loss at step 2500: 0.500493\n",
            "Train positive_sample_loss at step 2600: 0.487950\n",
            "Train negative_sample_loss at step 2600: 0.500574\n",
            "Train loss at step 2600: 0.494262\n",
            "Train positive_sample_loss at step 2700: 0.484552\n",
            "Train negative_sample_loss at step 2700: 0.497652\n",
            "Train loss at step 2700: 0.491102\n",
            "Train positive_sample_loss at step 2800: 0.479645\n",
            "Train negative_sample_loss at step 2800: 0.495659\n",
            "Train loss at step 2800: 0.487652\n",
            "Train positive_sample_loss at step 2900: 0.476404\n",
            "Train negative_sample_loss at step 2900: 0.493479\n",
            "Train loss at step 2900: 0.484941\n",
            "Train positive_sample_loss at step 3000: 0.476056\n",
            "Train negative_sample_loss at step 3000: 0.492863\n",
            "Train loss at step 3000: 0.484459\n",
            "Train positive_sample_loss at step 3100: 0.471193\n",
            "Train negative_sample_loss at step 3100: 0.488987\n",
            "Train loss at step 3100: 0.480090\n",
            "Train positive_sample_loss at step 3200: 0.468434\n",
            "Train negative_sample_loss at step 3200: 0.485698\n",
            "Train loss at step 3200: 0.477066\n",
            "Train positive_sample_loss at step 3300: 0.466563\n",
            "Train negative_sample_loss at step 3300: 0.486096\n",
            "Train loss at step 3300: 0.476330\n",
            "Train positive_sample_loss at step 3400: 0.461742\n",
            "Train negative_sample_loss at step 3400: 0.484495\n",
            "Train loss at step 3400: 0.473118\n",
            "Train positive_sample_loss at step 3500: 0.460326\n",
            "Train negative_sample_loss at step 3500: 0.483551\n",
            "Train loss at step 3500: 0.471938\n",
            "Train positive_sample_loss at step 3600: 0.457003\n",
            "Train negative_sample_loss at step 3600: 0.479889\n",
            "Train loss at step 3600: 0.468446\n",
            "Train positive_sample_loss at step 3700: 0.458370\n",
            "Train negative_sample_loss at step 3700: 0.479327\n",
            "Train loss at step 3700: 0.468848\n",
            "Train positive_sample_loss at step 3800: 0.451550\n",
            "Train negative_sample_loss at step 3800: 0.476578\n",
            "Train loss at step 3800: 0.464064\n",
            "Train positive_sample_loss at step 3900: 0.451427\n",
            "Train negative_sample_loss at step 3900: 0.474829\n",
            "Train loss at step 3900: 0.463128\n",
            "Train positive_sample_loss at step 4000: 0.450689\n",
            "Train negative_sample_loss at step 4000: 0.477845\n",
            "Train loss at step 4000: 0.464267\n",
            "Train positive_sample_loss at step 4100: 0.447566\n",
            "Train negative_sample_loss at step 4100: 0.474040\n",
            "Train loss at step 4100: 0.460803\n",
            "Train positive_sample_loss at step 4200: 0.443554\n",
            "Train negative_sample_loss at step 4200: 0.471806\n",
            "Train loss at step 4200: 0.457680\n",
            "Train positive_sample_loss at step 4300: 0.442435\n",
            "Train negative_sample_loss at step 4300: 0.470990\n",
            "Train loss at step 4300: 0.456712\n",
            "Train positive_sample_loss at step 4400: 0.441025\n",
            "Train negative_sample_loss at step 4400: 0.472087\n",
            "Train loss at step 4400: 0.456556\n",
            "Train positive_sample_loss at step 4500: 0.439136\n",
            "Train negative_sample_loss at step 4500: 0.469681\n",
            "Train loss at step 4500: 0.454409\n",
            "Train positive_sample_loss at step 4600: 0.439338\n",
            "Train negative_sample_loss at step 4600: 0.471109\n",
            "Train loss at step 4600: 0.455223\n",
            "Train positive_sample_loss at step 4700: 0.437234\n",
            "Train negative_sample_loss at step 4700: 0.469766\n",
            "Train loss at step 4700: 0.453500\n",
            "Train positive_sample_loss at step 4800: 0.437976\n",
            "Train negative_sample_loss at step 4800: 0.471731\n",
            "Train loss at step 4800: 0.454854\n",
            "Train positive_sample_loss at step 4900: 0.434944\n",
            "Train negative_sample_loss at step 4900: 0.465811\n",
            "Train loss at step 4900: 0.450377\n",
            "Train positive_sample_loss at step 5000: 0.435770\n",
            "Train negative_sample_loss at step 5000: 0.467234\n",
            "Train loss at step 5000: 0.451502\n",
            "Train positive_sample_loss at step 5100: 0.435223\n",
            "Train negative_sample_loss at step 5100: 0.469974\n",
            "Train loss at step 5100: 0.452599\n",
            "Train positive_sample_loss at step 5200: 0.435971\n",
            "Train negative_sample_loss at step 5200: 0.469324\n",
            "Train loss at step 5200: 0.452647\n",
            "Train positive_sample_loss at step 5300: 0.431831\n",
            "Train negative_sample_loss at step 5300: 0.465638\n",
            "Train loss at step 5300: 0.448734\n",
            "Train positive_sample_loss at step 5400: 0.430220\n",
            "Train negative_sample_loss at step 5400: 0.467042\n",
            "Train loss at step 5400: 0.448631\n",
            "Train positive_sample_loss at step 5500: 0.428736\n",
            "Train negative_sample_loss at step 5500: 0.463754\n",
            "Train loss at step 5500: 0.446245\n",
            "Train positive_sample_loss at step 5600: 0.430785\n",
            "Train negative_sample_loss at step 5600: 0.465128\n",
            "Train loss at step 5600: 0.447957\n",
            "Train positive_sample_loss at step 5700: 0.429036\n",
            "Train negative_sample_loss at step 5700: 0.466615\n",
            "Train loss at step 5700: 0.447826\n",
            "Train positive_sample_loss at step 5800: 0.426686\n",
            "Train negative_sample_loss at step 5800: 0.462802\n",
            "Train loss at step 5800: 0.444744\n",
            "Train positive_sample_loss at step 5900: 0.429342\n",
            "Train negative_sample_loss at step 5900: 0.468644\n",
            "Train loss at step 5900: 0.448993\n",
            "Train positive_sample_loss at step 6000: 0.426244\n",
            "Train negative_sample_loss at step 6000: 0.462686\n",
            "Train loss at step 6000: 0.444465\n",
            "Train positive_sample_loss at step 6100: 0.426587\n",
            "Train negative_sample_loss at step 6100: 0.462505\n",
            "Train loss at step 6100: 0.444546\n",
            "Train positive_sample_loss at step 6200: 0.424908\n",
            "Train negative_sample_loss at step 6200: 0.462333\n",
            "Train loss at step 6200: 0.443621\n",
            "Train positive_sample_loss at step 6300: 0.426407\n",
            "Train negative_sample_loss at step 6300: 0.465027\n",
            "Train loss at step 6300: 0.445717\n",
            "Train positive_sample_loss at step 6400: 0.423683\n",
            "Train negative_sample_loss at step 6400: 0.462592\n",
            "Train loss at step 6400: 0.443137\n",
            "Train positive_sample_loss at step 6500: 0.421145\n",
            "Train negative_sample_loss at step 6500: 0.463020\n",
            "Train loss at step 6500: 0.442082\n",
            "Train positive_sample_loss at step 6600: 0.423202\n",
            "Train negative_sample_loss at step 6600: 0.462284\n",
            "Train loss at step 6600: 0.442743\n",
            "Train positive_sample_loss at step 6700: 0.421734\n",
            "Train negative_sample_loss at step 6700: 0.463928\n",
            "Train loss at step 6700: 0.442831\n",
            "Train positive_sample_loss at step 6800: 0.418837\n",
            "Train negative_sample_loss at step 6800: 0.460698\n",
            "Train loss at step 6800: 0.439768\n",
            "Train positive_sample_loss at step 6900: 0.421184\n",
            "Train negative_sample_loss at step 6900: 0.461910\n",
            "Train loss at step 6900: 0.441547\n",
            "Train positive_sample_loss at step 7000: 0.420764\n",
            "Train negative_sample_loss at step 7000: 0.461751\n",
            "Train loss at step 7000: 0.441258\n",
            "Train positive_sample_loss at step 7100: 0.418150\n",
            "Train negative_sample_loss at step 7100: 0.459803\n",
            "Train loss at step 7100: 0.438976\n",
            "Train positive_sample_loss at step 7200: 0.420503\n",
            "Train negative_sample_loss at step 7200: 0.460932\n",
            "Train loss at step 7200: 0.440717\n",
            "Train positive_sample_loss at step 7300: 0.418216\n",
            "Train negative_sample_loss at step 7300: 0.459739\n",
            "Train loss at step 7300: 0.438977\n",
            "Train positive_sample_loss at step 7400: 0.417974\n",
            "Train negative_sample_loss at step 7400: 0.459188\n",
            "Train loss at step 7400: 0.438581\n",
            "Train positive_sample_loss at step 7500: 0.417134\n",
            "Train negative_sample_loss at step 7500: 0.460612\n",
            "Train loss at step 7500: 0.438873\n",
            "Train positive_sample_loss at step 7600: 0.416646\n",
            "Train negative_sample_loss at step 7600: 0.460577\n",
            "Train loss at step 7600: 0.438612\n",
            "Train positive_sample_loss at step 7700: 0.418026\n",
            "Train negative_sample_loss at step 7700: 0.461441\n",
            "Train loss at step 7700: 0.439733\n",
            "Train positive_sample_loss at step 7800: 0.418233\n",
            "Train negative_sample_loss at step 7800: 0.459896\n",
            "Train loss at step 7800: 0.439064\n",
            "Train positive_sample_loss at step 7900: 0.415651\n",
            "Train negative_sample_loss at step 7900: 0.459202\n",
            "Train loss at step 7900: 0.437426\n",
            "Train positive_sample_loss at step 8000: 0.413454\n",
            "Train negative_sample_loss at step 8000: 0.461451\n",
            "Train loss at step 8000: 0.437452\n",
            "Train positive_sample_loss at step 8100: 0.415897\n",
            "Train negative_sample_loss at step 8100: 0.462410\n",
            "Train loss at step 8100: 0.439154\n",
            "Train positive_sample_loss at step 8200: 0.415301\n",
            "Train negative_sample_loss at step 8200: 0.458761\n",
            "Train loss at step 8200: 0.437031\n",
            "Train positive_sample_loss at step 8300: 0.413139\n",
            "Train negative_sample_loss at step 8300: 0.460210\n",
            "Train loss at step 8300: 0.436674\n",
            "Train positive_sample_loss at step 8400: 0.413291\n",
            "Train negative_sample_loss at step 8400: 0.459485\n",
            "Train loss at step 8400: 0.436388\n",
            "Train positive_sample_loss at step 8500: 0.414855\n",
            "Train negative_sample_loss at step 8500: 0.459365\n",
            "Train loss at step 8500: 0.437110\n",
            "Train positive_sample_loss at step 8600: 0.415507\n",
            "Train negative_sample_loss at step 8600: 0.460752\n",
            "Train loss at step 8600: 0.438129\n",
            "Train positive_sample_loss at step 8700: 0.412289\n",
            "Train negative_sample_loss at step 8700: 0.458466\n",
            "Train loss at step 8700: 0.435377\n",
            "Train positive_sample_loss at step 8800: 0.416324\n",
            "Train negative_sample_loss at step 8800: 0.461455\n",
            "Train loss at step 8800: 0.438890\n",
            "Train positive_sample_loss at step 8900: 0.418183\n",
            "Train negative_sample_loss at step 8900: 0.462523\n",
            "Train loss at step 8900: 0.440353\n",
            "Train positive_sample_loss at step 9000: 0.411309\n",
            "Train negative_sample_loss at step 9000: 0.458273\n",
            "Train loss at step 9000: 0.434791\n",
            "Train positive_sample_loss at step 9100: 0.414096\n",
            "Train negative_sample_loss at step 9100: 0.461972\n",
            "Train loss at step 9100: 0.438034\n",
            "Train positive_sample_loss at step 9200: 0.411320\n",
            "Train negative_sample_loss at step 9200: 0.458504\n",
            "Train loss at step 9200: 0.434912\n",
            "Train positive_sample_loss at step 9300: 0.411946\n",
            "Train negative_sample_loss at step 9300: 0.461446\n",
            "Train loss at step 9300: 0.436696\n",
            "Train positive_sample_loss at step 9400: 0.411882\n",
            "Train negative_sample_loss at step 9400: 0.460008\n",
            "Train loss at step 9400: 0.435945\n",
            "Train positive_sample_loss at step 9500: 0.413925\n",
            "Train negative_sample_loss at step 9500: 0.461550\n",
            "Train loss at step 9500: 0.437738\n",
            "Train positive_sample_loss at step 9600: 0.411564\n",
            "Train negative_sample_loss at step 9600: 0.458795\n",
            "Train loss at step 9600: 0.435179\n",
            "Train positive_sample_loss at step 9700: 0.413317\n",
            "Train negative_sample_loss at step 9700: 0.458569\n",
            "Train loss at step 9700: 0.435943\n",
            "Train positive_sample_loss at step 9800: 0.411760\n",
            "Train negative_sample_loss at step 9800: 0.458638\n",
            "Train loss at step 9800: 0.435199\n",
            "Train positive_sample_loss at step 9900: 0.409913\n",
            "Train negative_sample_loss at step 9900: 0.458949\n",
            "Train loss at step 9900: 0.434431\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-19-c6f93b2030e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'pRotatE'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdouble_entity_embedding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-17-b382158aee00>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(model, double_entity_embedding, hidden_dim, gamma)\u001b[0m\n\u001b[1;32m    166\u001b[0m                   \u001b[0;34m'entity_dict'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mentity_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m               }\n\u001b[0;32m--> 168\u001b[0;31m               \u001b[0msave_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkge_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_variable_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'log_steps'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'save_model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We discuss in our [blog post](https://medium.com/@seshwan2/de5acf0553ac) the final results of training. The final output is that we will have our entities and relations embedded in 1000-dimensional vectors. These vectors are representations of our entities and relations and can be used, according to our models, to make link predictions and perform graph completion tasks."
      ],
      "metadata": {
        "id": "ZhDHZheaLJXv"
      }
    }
  ]
}